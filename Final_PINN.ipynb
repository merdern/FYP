{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "5UEsWpkPcp2X"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import matplotlib.pyplot as plt\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "import pandas as pd\n",
        "import time\n",
        "from scipy.interpolate import interp2d\n",
        "import plotly.graph_objects as go\n",
        "from plotly.subplots import make_subplots\n",
        "import plotly.express as px\n",
        "import torch.nn.init as init\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 111,
      "metadata": {
        "id": "TwNCrGQldjrH"
      },
      "outputs": [],
      "source": [
        "class Network(nn.Module):\n",
        "\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    # optimisers    \n",
        "\n",
        "    # constants and stuff\n",
        "    device = 'cuda'\n",
        "    self.N = 500\n",
        "    self.N_data = 500\n",
        "    self.r0_boundary, self.rs_boundary, self.t_boundaries, self.t0_boundary, self.r_boundary = self.init_boundaries(self.N)\n",
        "    self.r_data, self.t_data = self.sample(self.N_data)\n",
        "    self.c_numerical = torch.tensor(self.NumericalSoln(self.r_data.cpu().detach().numpy(), self.t_data.cpu().detach().numpy())).to(device)\n",
        "    self.j = -5.35e-5\n",
        "    self.Rs_ye = torch.tensor(2e-6).requires_grad_(True).to(device)\n",
        "    self.Rs = torch.tensor(2e-6)\n",
        "    self.c_max = 4.665e4\n",
        "    self.c_max_ye = torch.tensor(4.665e4).requires_grad_(True).to(device)\n",
        "    self.c0 = torch.tensor(2e4).view(-1, 1).to(device)\n",
        "    self.D_ref = 2e-16\n",
        "    self.C_theory = 277.84\n",
        "    self.C_practical = 160\n",
        "    self.t_max_ye = torch.tensor(400.).requires_grad_(True).to(device)\n",
        "\n",
        "    # storing D and loss values\n",
        "    self.Ds = []\n",
        "    self.loss_list = []\n",
        "    self.r0_loss_list = []\n",
        "    self.rs_loss_list = []\n",
        "    self.phys_loss_list = []\n",
        "    self.comp_loss_list = []\n",
        "\n",
        "    # nn constants\n",
        "    activation = nn.SELU\n",
        "    D_activation = nn.SELU\n",
        "    self.D_hidden = 16 # number of neurons in hidden layers for D\n",
        "    self.D_layers = 5 # number of hidden layers for D\n",
        "    self.input = 2 # input is r and t\n",
        "    self.output = 1 # output is c\n",
        "    self.layers = 10 # number of hidden layers\n",
        "    self.hidden = 128 # number of neurons in hidden layers\n",
        "\n",
        "    # c network\n",
        "    self.fci = nn.Sequential(*[nn.Linear(self.input, self.hidden), activation()])\n",
        "    init.kaiming_normal_(self.fci[0].weight, nonlinearity='tanh')\n",
        "    self.fch = nn.Sequential(*[nn.Sequential(*[nn.Linear(self.hidden, self.hidden), activation()]) for i in range(self.layers-1)])\n",
        "    for layer in self.fch.children():\n",
        "      if isinstance(layer, nn.Linear):\n",
        "          init.kaiming_normal_(layer.weight, nonlinearity='tanh')\n",
        "    self.fco = nn.Linear(self.hidden, self.output)\n",
        "\n",
        "    # D network\n",
        "    self.fciD = nn.Sequential(*[nn.Linear(1, self.D_hidden), D_activation()])\n",
        "    init.kaiming_normal_(self.fciD[0].weight)\n",
        "    self.fchD = nn.Sequential(*[nn.Sequential(*[nn.Linear(self.D_hidden, self.D_hidden), D_activation()]) for i in range(self.D_layers-1)])\n",
        "    for layer in self.fchD.children():\n",
        "      if isinstance(layer, nn.Linear):\n",
        "          init.kaiming_normal_(layer.weight)\n",
        "    self.fcoD = nn.Linear(self.D_hidden, 1)\n",
        "\n",
        "  # c forward propagation\n",
        "  def forward_c(self, x, t):\n",
        "    x = torch.cat((x, t), dim = 1)\n",
        "    x = self.fci(x)\n",
        "    x = self.fch(x)\n",
        "    x = self.fco(x)\n",
        "    x = self.c0/self.c_max + t**2*x\n",
        "    return x\n",
        "  \n",
        "  # D forward propagation\n",
        "  def forward_D(self, x):\n",
        "    x = self.fciD(x)\n",
        "    x = self.fchD(x)\n",
        "    x = self.fcoD(x)\n",
        "    x = x*1e-13\n",
        "    return x\n",
        "  \n",
        "  # numerical solution of spherical diffusion equation\n",
        "  def NumericalSoln(self, r_data, t_data):\n",
        "\n",
        "    numerical_sol = pd.read_csv(\"MATLAB Solver\\data.csv\")\n",
        "    \n",
        "    sol = []\n",
        "    for i in range(len(r_data)):\n",
        "      r = np.round(9999 * r_data[i]).astype(int)[0]\n",
        "      t = np.round(399 * t_data[i]).astype(int)[0]  \n",
        "      \n",
        "      c = numerical_sol.iloc[r, t]\n",
        "      sol.append(c)\n",
        "    return sol\n",
        "  \n",
        "  def init_boundaries(self, N):\n",
        "\n",
        "    # x and t boundary points for training x boundaries, x = 0,1, varying t values\n",
        "    r0_boundary = torch.tensor(0.).repeat(N).view(-1, 1).requires_grad_(True).to(device)\n",
        "    rs_boundary = torch.tensor(1.).repeat(N).view(-1, 1).requires_grad_(True).to(device)\n",
        "    t_boundaries = torch.rand(N).view(-1, 1).requires_grad_(True).to(device)\n",
        "\n",
        "    # 0 time boundary for training\n",
        "    t0_boundary = torch.tensor(0.).repeat(N).view(-1, 1).requires_grad_(True).to(device)\n",
        "    r_boundary = torch.rand(N).view(-1, 1).requires_grad_(True).to(device)\n",
        "\n",
        "    return r0_boundary, rs_boundary, t_boundaries, t0_boundary, r_boundary\n",
        "\n",
        "  def sample(self, N):\n",
        "        \n",
        "    # sample points for training\n",
        "    r = torch.rand(N).view(-1, 1).requires_grad_(True).to(device).view(-1, 1)\n",
        "    t = torch.rand(N).view(-1, 1).requires_grad_(True).to(device).view(-1, 1)\n",
        "\n",
        "    return r, t\n",
        "\n",
        "# adaptive sampling, unused due to VRAM limitations\n",
        "  def adaptive_sample(self, N):\n",
        "    \n",
        "    r = torch.linspace(0.0001, 1, 2*N).requires_grad_(True).to(device)\n",
        "    t = torch.linspace(0.0001, 1, 2*N).requires_grad_(True).to(device)\n",
        "    r_grid, t_grid = torch.meshgrid(r, t)\n",
        "    phys_loss = self.phys_loss(r_grid.flatten().unsqueeze(1), t_grid.flatten().unsqueeze(1))\n",
        "\n",
        "    ids = torch.multinomial(phys_loss, N, replacement=True)\n",
        "    r = r[ids]\n",
        "    t = t[ids]\n",
        "\n",
        "    print(r.shape, t.shape)\n",
        "\n",
        "    return r, t\n",
        "\n",
        "  def Diffusion_Coeff(self, c):\n",
        "\n",
        "    SOC = (self.c_max - c)/self.c_max * self.C_theory/self.C_practical\n",
        "    D = self.D_ref * (1 + 100*SOC**(3/2))\n",
        "\n",
        "    return D\n",
        "  \n",
        "  def deriv_t(self, c, t):\n",
        "    return torch.autograd.grad(c, t, grad_outputs=torch.ones_like(c).to(device), create_graph=True)[0]\n",
        "  \n",
        "  def deriv_r(self, c, r):\n",
        "    return torch.autograd.grad(c, r, grad_outputs=torch.ones_like(c).to(device), create_graph=True)[0]\n",
        "  \n",
        "  def phys_loss(self, r, t):\n",
        "\n",
        "    Rs = self.Rs\n",
        "    c = self.forward_c(r, t)\n",
        "    c_r = self.deriv_r(c, r)\n",
        "    c_rr = self.deriv_r(c_r, r)\n",
        "    c_t = self.deriv_t(c, t)\n",
        "    D = self.forward_D(c)\n",
        "    D_r = self.deriv_r(D, r)\n",
        "    term = r**2*D*c_r/(Rs**2)\n",
        "    deriv = self.deriv_r(term, r)\n",
        "\n",
        "    #loss = (r**2*c_t - (2*r*c_r - D*r**2*c_rr - D_r*r**2*c_r)/(Rs**2))**2\n",
        "    loss = (r**2*c_t - deriv)**2\n",
        "    return loss\n",
        "\n",
        "  def losses(self):\n",
        "    \n",
        "    r_phys, t_phys = self.sample(self.N)\n",
        "    j, Rs, c_max, c0 = self.j, self.Rs, self.c_max, self.c0 \n",
        "\n",
        "    # loss for r = 0 boundary\n",
        "    c = self.forward_c(self.r0_boundary, self.t_boundaries)\n",
        "    c_r = self.deriv_r(c, self.r0_boundary)\n",
        "    r0_loss = c_r**2\n",
        "    self.r0_loss_list.append(torch.mean(r0_loss).item())\n",
        "    \n",
        "    # loss for x = rs boundary\n",
        "    c = self.forward_c(self.rs_boundary, self.t_boundaries)\n",
        "    c_r = self.deriv_r(c, self.rs_boundary)\n",
        "    D = self.forward_D(c)\n",
        "    rs_loss = (D*c_r + j*Rs/c_max)**2\n",
        "    self.rs_loss_list.append(torch.mean(rs_loss).item())\n",
        "\n",
        "    # physics loss\n",
        "    phys_loss = self.phys_loss(r_phys, t_phys)\n",
        "    self.phys_loss_list.append(torch.mean(phys_loss).item())\n",
        "    \n",
        "    # comparative loss\n",
        "    c = self.c_max*self.forward_c(self.r_data, self.t_data)\n",
        "    comp_loss = 1/self.c_max*(torch.sub(c, self.c_numerical.unsqueeze(1)))**2\n",
        "    self.comp_loss_list.append(torch.mean(comp_loss).item())\n",
        "\n",
        "    loss = torch.mean(r0_loss) + 1e28*torch.mean(rs_loss) + 100*torch.mean(phys_loss) + 100*torch.mean(comp_loss)\n",
        "    #100*torch.mean(phys_loss)\n",
        "    print(f'epoch: {self.epoch}, pde loss: {100*torch.mean(phys_loss).item()}, r0 loss: {torch.mean(r0_loss).item()}, rs loss: {1e28*torch.mean(rs_loss).item()}, comp loss:{torch.mean(comp_loss).item()})', end='\\r')\n",
        "\n",
        "\n",
        "    return loss\n",
        "  \n",
        "  def closure(self):\n",
        "    self.optimizer_lbfgs.zero_grad()\n",
        "    loss = self.losses()\n",
        "    loss.backward()\n",
        "    return loss\n",
        "  \n",
        "  def train(self, epochs_adam, lr_adam, epochs_lbfgs):\n",
        "\n",
        "    # Adam\n",
        "    optimizer_adam = torch.optim.Adam(self.parameters(), lr = lr_adam)\n",
        "    for epoch in range(epochs_adam):\n",
        "\n",
        "      self.epoch = epoch\n",
        "      \n",
        "      optimizer_adam.zero_grad()\n",
        "      loss = self.losses()\n",
        "      self.loss_list.append(loss.item())\n",
        "      loss.backward()\n",
        "      #torch.nn.utils.clip_grad_norm_(self.parameters(), 1)\n",
        "      optimizer_adam.step()\n",
        "      #print(f'Epoch: {epoch}  Loss: {loss.item()}', end='\\r')\n",
        "\n",
        "    # LBFGS\n",
        "    self.optimizer_lbfgs = torch.optim.LBFGS(self.parameters(), \n",
        "                                        max_iter=400, \n",
        "                                        history_size=400, \n",
        "                                        tolerance_grad=1.0 * np.finfo(float).eps, \n",
        "                                        tolerance_change=1.0 * np.finfo(float).eps, \n",
        "                                        line_search_fn='strong_wolfe')\n",
        "    for epoch in range(epochs_lbfgs):\n",
        "\n",
        "      self.loss_list.append(loss.item())\n",
        "      loss = self.optimizer_lbfgs.step(self.closure)\n",
        "\n",
        "      self.epoch = epochs_adam + epoch\n",
        "\n",
        "      #print(f'Epoch: {epochs_adam + epoch}  Loss: {loss.item()}', end='\\r')\n",
        "\n",
        "    self.plot()\n",
        "    error_D = self.plot_D()\n",
        "    self.plot_loss()\n",
        "    self.plot_comparison()\n",
        "    error_c = self.error()\n",
        "\n",
        "    return self.loss_list, error_D, error_c\n",
        "\n",
        "  def plot(self):\n",
        "\n",
        "      X = torch.linspace(0, 1, 100).to(device)\n",
        "      T = torch.linspace(0, 1, 100).to(device)\n",
        "      X_grid, T_grid = torch.meshgrid(X, T)\n",
        "      \n",
        "      C = self.c_max*self.forward_c(X_grid.flatten().unsqueeze(1), T_grid.flatten().unsqueeze(1))\n",
        "      C = C.view(100, 100)\n",
        "      C = C.cpu().detach().numpy()\n",
        "      \n",
        "      X, T = np.meshgrid(self.Rs*X.cpu().detach().numpy(), 400*T.cpu().detach().numpy())\n",
        "\n",
        "      fig = go.Figure(data=[go.Surface(z=C, x=X, y=T)])\n",
        "      fig.update_layout(template = 'plotly_dark', \n",
        "                        title='Concentration vs Time and Radius', \n",
        "                        scene = dict(xaxis = dict(title = 'Radius'),\n",
        "                        yaxis = dict(title = 'Time (s)'),\n",
        "                        zaxis = dict(title = 'Concentration')),                        \n",
        "                        autosize=False, width=800, height=600, margin=dict(l=50, r=50, t=50, b=50))\n",
        "      fig.show()\n",
        "\n",
        "  def plot_D(self):\n",
        "\n",
        "    c = torch.linspace(0, 1, 100).view(-1, 1).to(device)\n",
        "    D = self.forward_D(c).cpu().detach().numpy()\n",
        "    c = self.c_max*c.cpu().detach().numpy()\n",
        "    D_SOC = self.Diffusion_Coeff(c)\n",
        "\n",
        "    error_D = np.mean(np.sqrt((D - D_SOC)**2))\n",
        "\n",
        "    plt.plot(c.flatten(), D.flatten(), label='Diffusion Coefficient Network', color='blue')\n",
        "    plt.plot(c.flatten(), D_SOC.flatten(), label='Diffusion Coefficient Analytical', color='red')\n",
        "\n",
        "    # Set plot title and labels\n",
        "    plt.title('Diffusion Coefficient vs Concentration')\n",
        "    plt.xlabel('Concentration')\n",
        "    plt.ylabel('Diffusion Coefficient')\n",
        "    plt.yscale('log')\n",
        "\n",
        "    # Add legend\n",
        "    plt.legend()\n",
        "\n",
        "    # Show plot\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "    return error_D\n",
        "\n",
        "  def plot_loss(self):\n",
        "    fig = px.scatter(\n",
        "            x=range(len(self.loss_list)),\n",
        "            y=self.loss_list,\n",
        "            log_y=True,\n",
        "            log_x=True,\n",
        "            color=np.log(self.loss_list),\n",
        "            color_continuous_scale=\"Agsunset\",\n",
        "            labels={\"x\": \"Epochs\", \"y\": \"Loss\"},\n",
        "            template='plotly_dark'\n",
        "        )\n",
        "    fig.show()\n",
        "\n",
        "    # Create a figure and subplots\n",
        "    fig, axs = plt.subplots(1, 4, figsize=(12, 6))\n",
        "\n",
        "    # Plot r0_loss\n",
        "    axs[0].plot(self.r0_loss_list, label='r0_loss')\n",
        "    axs[0].set_ylabel('r0_loss')\n",
        "    axs[0].set_yscale('log')  \n",
        "    axs[0].set_xlabel('Epochs')\n",
        "    axs[0].set_title('R0 Loss vs Epochs')\n",
        "    axs[0].grid(True)\n",
        "\n",
        "    # Plot rs_loss\n",
        "    axs[1].plot(self.rs_loss_list, label='rs_loss')\n",
        "    axs[1].set_ylabel('rs_loss')\n",
        "    axs[1].set_yscale('log')\n",
        "    axs[1].set_xlabel('Epochs')\n",
        "    axs[1].set_title('Rs Loss vs Epochs')\n",
        "    axs[1].grid(True)\n",
        "\n",
        "    # Plot phys_loss\n",
        "    axs[2].plot(self.phys_loss_list, label='phys_loss')\n",
        "    axs[2].set_xlabel('Epochs')\n",
        "    axs[2].set_yscale('log')\n",
        "    axs[2].set_ylabel('phys_loss')\n",
        "    axs[2].set_title('Physics Loss vs Epochs')\n",
        "    axs[2].grid(True)\n",
        "\n",
        "    # Plot comp_loss\n",
        "    axs[3].plot(self.comp_loss_list, label='comp_loss')\n",
        "    axs[3].set_xlabel('Epochs')\n",
        "    axs[3].set_yscale('log')\n",
        "    axs[3].set_ylabel('comp_loss')\n",
        "    axs[3].set_title('Comparative Loss vs Epochs')\n",
        "    axs[3].grid(True)\n",
        "    \n",
        "    # Adjust layout\n",
        "    plt.tight_layout()\n",
        "\n",
        "    # Show the plot\n",
        "    plt.show()\n",
        "\n",
        "  def plot_comparison(self):\n",
        "      numerical = pd.read_csv(\"MATLAB Solver\\data.csv\")\n",
        "      X = torch.linspace(0, 1, 100).to(device).requires_grad_(True)\n",
        "      T = torch.linspace(0, 1, 100).to(device).requires_grad_(True)\n",
        "      X_grid, T_grid = torch.meshgrid(X, T)\n",
        "\n",
        "      C = self.c_max * self.forward_c(X_grid.flatten().unsqueeze(1), T_grid.flatten().unsqueeze(1))\n",
        "      C = C.view(100, 100).cpu().detach().numpy()\n",
        "\n",
        "      X = np.linspace(0, 10000, 100)\n",
        "      T = np.linspace(0, 400, 100)\n",
        "\n",
        "      fig, axs = plt.subplots(2, 2, figsize=(10, 10))\n",
        "\n",
        "      axs[0, 0].plot(X, C[:, 0], label='NN', color=\"blue\", marker = 'o',markevery=10)\n",
        "      axs[0, 0].plot(numerical.iloc[:, 0], label='Numerical', color=\"red\")\n",
        "      axs[0, 0].set_title('Comparison at t=0')\n",
        "      axs[0, 0].set_xlabel('X')\n",
        "      axs[0, 0].set_ylabel('Concentration (mol/m³)')\n",
        "      axs[0, 0].legend()\n",
        "\n",
        "      axs[0, 1].plot(X, C[:, -1], label='NN', color=\"blue\", marker = 'o',markevery=10)\n",
        "      axs[0, 1].plot(numerical.iloc[:, -1], label='Numerical', color=\"red\")\n",
        "      axs[0, 1].set_title('Comparison at t=end')\n",
        "      axs[0, 1].set_xlabel('X')\n",
        "      axs[0, 1].set_ylabel('Concentration (mol/m³)')\n",
        "\n",
        "      axs[1, 0].plot(T, C[0, :], label='NN', color=\"blue\", marker = 'o',markevery=10)\n",
        "      axs[1, 0].plot(numerical.iloc[0, :], label='Numerical', color=\"red\")\n",
        "      axs[1, 0].set_title('Comparison at r=0')\n",
        "      axs[1, 0].set_xlabel('T')\n",
        "      axs[1, 0].set_ylabel('Concentration (mol/m³)')\n",
        "\n",
        "      axs[1, 1].plot(T, C[-1, :], label='NN', color=\"blue\", marker = 'o',markevery=10)\n",
        "      axs[1, 1].plot(numerical.iloc[-1, :], label='Numerical', color=\"red\")\n",
        "      axs[1, 1].set_title('Comparison at r=end')\n",
        "      axs[1, 1].set_xlabel('T')\n",
        "      axs[1, 1].set_ylabel('Concentration (mol/m³)')\n",
        "\n",
        "      plt.tight_layout()\n",
        "      plt.show()\n",
        "      \n",
        "  def error(self):\n",
        "    \n",
        "    X = torch.linspace(0, 0.99, 101).to(device).requires_grad_(True)\n",
        "    T = torch.linspace(0, 0.99, 101).to(device).requires_grad_(True)\n",
        "    X_grid, T_grid = torch.meshgrid(X, T)\n",
        "    C = self.forward_c(X_grid.flatten().unsqueeze(1), T_grid.flatten().unsqueeze(1))\n",
        "    C = self.c_max*C.view(101, 101)\n",
        "    X_int = np.round(9999*X.cpu().detach().numpy()).astype(int)\n",
        "    T_int = np.round(399*T.cpu().detach().numpy()).astype(int)\n",
        "\n",
        "    numerical = pd.read_csv(\"MATLAB Solver\\data.csv\").to_numpy()\n",
        "    numerical = numerical[np.ix_(X_int, T_int)]\n",
        "    error = np.absolute(numerical - C.cpu().detach().numpy())/numerical*100\n",
        "    abs_error = np.mean(np.sqrt((numerical - C.cpu().detach().numpy())**2))\n",
        "\n",
        "\n",
        "\n",
        "    phys_loss = self.phys_loss(X_grid.flatten().unsqueeze(1), T_grid.flatten().unsqueeze(1)).view(101, 101).cpu().detach().numpy()\n",
        "\n",
        "    fig, axs = plt.subplots(ncols = 2, figsize=(12, 5))\n",
        "\n",
        "    X, T = self.Rs*10000*X.cpu().detach().numpy(), 400*T.cpu().detach().numpy()\n",
        "\n",
        "      # Numerical plot\n",
        "    cs0 = axs[0].contourf(X, T, numerical.T, levels = 15)  # Transpose numerical data\n",
        "    fig.colorbar(cs0, ax=axs[0], label='Concentration (mol/m³)')\n",
        "    axs[0].set_title('Numerical Solution')\n",
        "    axs[0].set_xlabel('T (s)')  # Swap x and y labels\n",
        "    axs[0].set_ylabel('R (m)')  # Swap x and y labels\n",
        "    #axs[0].legend()\n",
        "\n",
        "    # C against X and T\n",
        "    cs1 = axs[1].contourf(X, T, C.cpu().detach().numpy().T, levels = 15)  # Transpose C data\n",
        "    fig.colorbar(cs1, ax=axs[1], label='Concentration (mol/m³)')\n",
        "    axs[1].set_title('Neural Network Solution')\n",
        "    axs[1].set_xlabel('T (s)')  # Swap x and y labels\n",
        "    axs[1].set_ylabel('R (m)')  # Swap x and y labels\n",
        "    #axs[1].legend()\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "    fig, axs = plt.subplots(ncols = 2, figsize=(10, 5))\n",
        "    # Error Surface\n",
        "    cs2 = axs[0].contourf(X, T, error.T, cmap = 'magma')  # Transpose error data\n",
        "    fig.colorbar(cs2, ax=axs[0], label='Error %')\n",
        "    axs[0].set_title('Error Surface')\n",
        "    axs[0].set_xlabel('T (s)')  # Swap x and y labels\n",
        "    axs[0].set_ylabel('R (m)')  # Swap x and y labels\n",
        "\n",
        "    # Physical Loss Surface\n",
        "    cs3 = axs[1].contourf(X, T, phys_loss.T, cmap = 'magma')  # Transpose phys_loss data\n",
        "    fig.colorbar(cs3, ax=axs[1], label='Physical Loss')\n",
        "    axs[1].set_title('Physical Loss Surface')\n",
        "    axs[1].set_xlabel('T (s)')  # Swap x and y labels\n",
        "    axs[1].set_ylabel('R (m)')  # Swap x and y labels\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    return abs_error\n",
        "    \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 110,
      "metadata": {
        "id": "C23NsLX_ZtTJ"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch: 346, pde loss: 2.022578939795494, r0 loss: 0.004455450922250748, rs loss: 0.021855298784047524, comp loss:2.104393482208252))))\r"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[110], line 13\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# define nn to train, two inputs for x and t\u001b[39;00m\n\u001b[0;32m     10\u001b[0m pinn \u001b[38;5;241m=\u001b[39m Network()\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m---> 13\u001b[0m losses \u001b[38;5;241m=\u001b[39m pinn\u001b[38;5;241m.\u001b[39mtrain(epochs_adam, lr_adam, epochs_lbfgs)\n",
            "Cell \u001b[1;32mIn[108], line 207\u001b[0m, in \u001b[0;36mNetwork.train\u001b[1;34m(self, epochs_adam, lr_adam, epochs_lbfgs)\u001b[0m\n\u001b[0;32m    204\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepoch \u001b[38;5;241m=\u001b[39m epoch\n\u001b[0;32m    206\u001b[0m optimizer_adam\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m--> 207\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlosses()\n\u001b[0;32m    208\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss_list\u001b[38;5;241m.\u001b[39mappend(loss\u001b[38;5;241m.\u001b[39mitem())\n\u001b[0;32m    209\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
            "Cell \u001b[1;32mIn[108], line 164\u001b[0m, in \u001b[0;36mNetwork.losses\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    161\u001b[0m j, Rs, c_max, c0 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mj, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mRs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mc_max, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mc0 \n\u001b[0;32m    163\u001b[0m \u001b[38;5;66;03m# loss for r = 0 boundary\u001b[39;00m\n\u001b[1;32m--> 164\u001b[0m c \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward_c(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mr0_boundary, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mt_boundaries)\n\u001b[0;32m    165\u001b[0m c_r \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mderiv_r(c, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mr0_boundary)\n\u001b[0;32m    166\u001b[0m r0_loss \u001b[38;5;241m=\u001b[39m c_r\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m\n",
            "Cell \u001b[1;32mIn[108], line 64\u001b[0m, in \u001b[0;36mNetwork.forward_c\u001b[1;34m(self, x, t)\u001b[0m\n\u001b[0;32m     62\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward_c\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, t):\n\u001b[0;32m     63\u001b[0m   x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat((x, t), dim \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m---> 64\u001b[0m   x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfci(x)\n\u001b[0;32m     65\u001b[0m   x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfch(x)\n\u001b[0;32m     66\u001b[0m   x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfco(x)\n",
            "File \u001b[1;32mc:\\Users\\twcy2\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
            "File \u001b[1;32mc:\\Users\\twcy2\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\twcy2\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m module(\u001b[38;5;28minput\u001b[39m)\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
            "File \u001b[1;32mc:\\Users\\twcy2\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
            "File \u001b[1;32mc:\\Users\\twcy2\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\twcy2\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\activation.py:607\u001b[0m, in \u001b[0;36mSELU.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    606\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 607\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mselu(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minplace)\n",
            "File \u001b[1;32mc:\\Users\\twcy2\\anaconda3\\Lib\\site-packages\\torch\\nn\\functional.py:1593\u001b[0m, in \u001b[0;36mselu\u001b[1;34m(input, inplace)\u001b[0m\n\u001b[0;32m   1591\u001b[0m     result \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mselu_(\u001b[38;5;28minput\u001b[39m)\n\u001b[0;32m   1592\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1593\u001b[0m     result \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mselu(\u001b[38;5;28minput\u001b[39m)\n\u001b[0;32m   1594\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# nn seed\n",
        "device = 'cuda'\n",
        "N = 300\n",
        "epochs_adam = 1000\n",
        "lr_adam = 1e-4\n",
        "epochs_lbfgs = 100\n",
        "\n",
        "\n",
        "# define nn to train, two inputs for x and t\n",
        "pinn = Network().to(device)\n",
        "\n",
        "\n",
        "losses = pinn.train(epochs_adam, lr_adam, epochs_lbfgs)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
